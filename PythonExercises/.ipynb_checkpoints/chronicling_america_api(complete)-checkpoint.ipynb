{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chronicling America API\n",
        "\n",
        "[Chronicling America](https://chroniclingamerica.loc.gov/) is a collection of digitized American newspapers dating from 1777 to 1963 provided by the Library of Congress. The collection offers an application programming interface (API) which allows users to easily harvest large amounts of data.\n",
        "\n",
        "In this notebook we will search Chronicling America's API, gather the search results into a Pandas dataframe, clean the data, and save it as a csv file."
      ],
      "metadata": {
        "id": "lg1_EIHEETuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0-jsD68ELup"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chronicling America URLs\n",
        "\n",
        "If I search for a term, \"abolition\" for example, on https://chroniclingamerica.loc.gov/ I will get a results url that looks like this:\n",
        "\n",
        "https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1770&date2=1963&proxtext=abolition&x=12&y=18&dateFilterType=yearRange&rows=20&searchType=basic\n",
        "\n",
        "These search results are human actionable, but not machine actionable. Chronicling America as an API that allows me to get machine actionable results if I add `&format=json`:\n",
        "\n",
        "https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1770&date2=1963&proxtext=abolition&x=12&y=18&dateFilterType=yearRange&rows=20&searchType=basic&format=json\n",
        "\n",
        "If we examine the url we see that there are a number of search parameters:\n",
        "- `state=`\n",
        "- `date1=1770`\n",
        "- `date2=1963`\n",
        "- `proxtext=abolition`\n",
        "\n",
        "We can edit these values to modify our search. I change the parameters to limit our search:\n",
        "\n",
        "https://chroniclingamerica.loc.gov/search/pages/results/?state=Massachusetts&date1=1770&date2=1865&proxtext=prohibition&x=20&y=8&dateFilterType=yearRange&rows=20&searchType=basic&format=json"
      ],
      "metadata": {
        "id": "XQFGCgdRHpCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can use the `requests` library to retrieve data from the url."
      ],
      "metadata": {
        "id": "8x69VEy1KCpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initial search\n",
        "url = 'https://chroniclingamerica.loc.gov/search/pages/results/?state=Massachusetts&date1=1770&date2=1865&proxtext=prohibition&x=20&y=8&dateFilterType=yearRange&rows=20&searchType=basic&format=json'\n",
        "response = requests.get(url)\n",
        "raw = response.text\n",
        "results = json.loads(raw)"
      ],
      "metadata": {
        "id": "uoseSCpoEghd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore search results"
      ],
      "metadata": {
        "id": "g-IV4qxIKlXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.keys()"
      ],
      "metadata": {
        "id": "Pg-9a2pIKTnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explore items\n",
        "print(type(results['items']))"
      ],
      "metadata": {
        "id": "jB1fHVUTEcLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results['items'][0])"
      ],
      "metadata": {
        "id": "LduCA0d1Etzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('totalItems:', results['totalItems'])\n",
        "print('endIndex:', results['endIndex'])\n",
        "print('startIndex:', results['startIndex'])\n",
        "print('itemsPerPage:', results['itemsPerPage'])\n",
        "print('Length and type of items:', len(results['items']), type(results['items']))"
      ],
      "metadata": {
        "id": "kIcPNqHuKtJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chronicling America API returned 1,656 results. However, it will only display 20 at a time by default. I can add a new parameter `page=` to cycle through all the results, but first I need to know how many pages there will be. I can find this out by dividing `totalItems` (1,656) by `itemsPerPage` (20) and then round-up using `math.ceil`."
      ],
      "metadata": {
        "id": "cxWJPQfUK5pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find total amount of pages\n",
        "total_pages = math.ceil(results['totalItems'] / results['itemsPerPage'])\n",
        "print(total_pages)"
      ],
      "metadata": {
        "id": "OmJIDL1lKy0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I know how many pages there will be, I can use a for loop to iterate through each result page and then each item on each result page. I then gather the data I want from each item: newspaper title, city, date, and text.\n",
        "\n",
        "Notice in the code below I placed the url string in parentheses () so that I could break it up over multiple lines making it easier to read.\n",
        "\n",
        "Also, for the sake of this demonstration, I am only iterating over 10 pages. For the full results the for loop should begin: `for i in range(1, total_pages+1)` (the `+1` is necessary becase the seond number in the range function is exclusive)."
      ],
      "metadata": {
        "id": "-Gca6IQYUEVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create empty list for data\n",
        "data = []"
      ],
      "metadata": {
        "id": "C0pZe96qBP_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set search parameters\n",
        "start_date = '1770'\n",
        "end_date = '1865'\n",
        "search_term = 'abolition'\n",
        "state = 'Massachusetts'"
      ],
      "metadata": {
        "id": "pg63qYBuBVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through search results and collect data\n",
        "for i in range(1, 11):  # for sake of time I'm doing only 10, you will want to put total_pages+1\n",
        "    url = (f'https://chroniclingamerica.loc.gov/search/pages/results/?state={state}&date1={start_date}'\n",
        "           f'&date2={end_date}&proxtext={search_term}&x=16&y=8&dateFilterType=yearRange&rows=20'\n",
        "           f'&searchType=basic&format=json&page={i}')  # f-string\n",
        "    response = requests.get(url)\n",
        "    raw = response.text\n",
        "    print(f'page {i} status code:', response.status_code)  # checking for errors\n",
        "    results = json.loads(raw)\n",
        "    items_ = results['items']\n",
        "    for item_ in items_:\n",
        "        row_data = {}\n",
        "        try:\n",
        "          row_data['title'] = item_['title_normal']\n",
        "        except:\n",
        "          row_data['city'] = \"none\"\n",
        "        try:\n",
        "          row_data['city'] = item_['city']\n",
        "        except:\n",
        "          row_data['city'] = \"none\"\n",
        "        try:\n",
        "          row_data['date'] = item_['date']\n",
        "        except:\n",
        "          row_data['date'] = \"none\"\n",
        "        try:\n",
        "          row_data['raw_text'] = item_['ocr_eng']\n",
        "        except:\n",
        "          row_data['raw_text'] = 'none'\n",
        "    data.append(row_data)"
      ],
      "metadata": {
        "id": "Upp6d0I9UDy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put data into DataFrame\n",
        "df = pd.DataFrame.from_dict(data)"
      ],
      "metadata": {
        "id": "q-ctFdtSBa-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "prL29Su_msjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change date format\n",
        "Pandas allows us to clean and edit our data easily (relatively). We can first convert the string values in the date column to properly formated dates and then sort the dataframe by date."
      ],
      "metadata": {
        "id": "z509dIQEep7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert date column from string to date-time object\n",
        "df['date'] = pd.to_datetime(df['date'])"
      ],
      "metadata": {
        "id": "z1fENcFaZJIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "4c0otcZIey1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort by date\n",
        "df = df.sort_values(by='date')"
      ],
      "metadata": {
        "id": "wjTfqq38e0XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "3XYFLmRhe7Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process text\n",
        "We can now porcess our text for analysis. The text provded by Chronicling America comes from optical character recognition (ocr) and the accuracy of ocr can be low. Here I will remove new line characters (`\\n`), stop words, and then lemamtize the text.\n",
        "\n",
        "**Rememeber** the decisions you make in how to process your text should be based on the kind of analysis you want to do."
      ],
      "metadata": {
        "id": "epH12SFJfJm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write fuction to process text\n",
        "# load nlp model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.disable_pipes('ner', 'parser')  # these are unnecessary for the task at hand\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"Remove new line characters and lemmatize text. Returns string of lemmas\"\"\"\n",
        "    text = text.replace('\\n', ' ')\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc]\n",
        "    no_stops = [token for token in tokens if not token.is_stop]\n",
        "    no_punct = [token for token in no_stops if token.is_alpha]\n",
        "    lemmas = [token.lemma_ for token in no_punct]\n",
        "    lemmas_lower = [lemma.lower() for lemma in lemmas]\n",
        "    lemmas_string = ' '.join(lemmas_lower)\n",
        "    return lemmas_string"
      ],
      "metadata": {
        "id": "m6Urrlffe8ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply process_text function\n",
        "# this may take a few minutes\n",
        "df['lemmas'] = df['raw_text'].apply(process_text)"
      ],
      "metadata": {
        "id": "VkQk9wuXfrwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save to csv\n",
        "df.to_csv(f'../data/{search_term}{start_date}-{end_date}.csv', index=False)"
      ],
      "metadata": {
        "id": "7UU3K6rkfsRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}